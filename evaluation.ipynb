{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd() == '/home/user/code':\n",
    "    os.chdir('/home/user/code/nlp2024_ClefTask4SOTA')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>f</th>\n",
       "      <th>annotation</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>inference_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1503.05062</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>21.783005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1109.0784</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>2.080140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2210.16422v1</td>\n",
       "      <td>[Here are the extracted Tasks, Datasets, Metri...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Text Summarization'...</td>\n",
       "      <td>27.891736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2310.07488v2</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Math Word Problems',...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Arithmetic Reasonin...</td>\n",
       "      <td>35.307533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2210.15425v1</td>\n",
       "      <td>[ {'LEADERBOARD':{ 'Task': 'Keyword Spotting',...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Keyword Spotting', ...</td>\n",
       "      <td>17.645482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2108.06107v1</td>\n",
       "      <td>[57.70, 53.77, 70.23, 69.17, 63.95, 56.37, 69....</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Aspect Sentiment Tr...</td>\n",
       "      <td>19.245811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1404.0089</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>12.290817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2205.10511v1</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'DocRE', 'Dataset': '...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Relation Extraction...</td>\n",
       "      <td>42.081739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2005.11849v2</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'English GEC', 'Datas...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Grammatical Error C...</td>\n",
       "      <td>37.168867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2009.01559v1</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Object Detection', '...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Instance Segmentati...</td>\n",
       "      <td>59.322940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2205.10455v2</td>\n",
       "      <td>[{'LEADERBOARD':{'Model': '+ SSP', 'Tasks': ['...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Question Answering'...</td>\n",
       "      <td>34.948370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2101.04727v1</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Textual Cloze Task',...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Question Answering'...</td>\n",
       "      <td>14.899040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2102.07624v1</td>\n",
       "      <td>[Here are the extracted Tasks, Datasets, Metri...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Action Spotting', '...</td>\n",
       "      <td>35.626044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2103.02603v1</td>\n",
       "      <td>[ {'LEADERBOARD':{ 'Task': 'ORE-(CC+EBUI)', 'D...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>41.838210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1711.07280v3</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Visually-Grounded Na...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Visual Navigation',...</td>\n",
       "      <td>26.552889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1208.6051</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>10.129432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>0802.2827</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>7.930030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1102.1935</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>1.791833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2004.05343v1</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': None, 'Dataset': ['Go...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Deblurring', 'Datas...</td>\n",
       "      <td>11.181190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>0809.1489</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Facial Expression Re...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>24.773965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>0906.3896</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Geometric Stabbing',...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>25.420012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1402.3314</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>1.287452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1303.6242</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'WSN simulation', 'Da...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>24.049734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1508.01441</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>4.730810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1402.2090</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>2.543730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1905.10295v6</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Few-shot learning', ...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Few-Shot Image Clas...</td>\n",
       "      <td>19.164492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>0705.1367</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>4.582883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2106.01223v1</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Named Entity Recogni...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Named Entity Recogn...</td>\n",
       "      <td>15.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1712.06113v3</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'MD simulation', 'Dat...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Formation Energy', ...</td>\n",
       "      <td>22.111865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>0802.2108</td>\n",
       "      <td>[Here are the extracted Tasks, Datasets, Metri...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>17.489531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2004.04572v2</td>\n",
       "      <td>[Here are the extracted benchmark results:**Ta...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': '3D Object Reconstru...</td>\n",
       "      <td>19.956842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>1403.6303</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>11.872271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2108.03348v3</td>\n",
       "      <td>[Here are the extracted benchmark results:**Ta...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Link Prediction', '...</td>\n",
       "      <td>47.365411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2109.04066v1</td>\n",
       "      <td>[{'LEADERBOARD':{'Task': 'Molweni', 'Dataset':...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Question Answering'...</td>\n",
       "      <td>21.563660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  run             f  \\\n",
       "0   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...    1503.05062   \n",
       "1   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     1109.0784   \n",
       "2   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2210.16422v1   \n",
       "3   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2310.07488v2   \n",
       "4   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2210.15425v1   \n",
       "5   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2108.06107v1   \n",
       "6   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     1404.0089   \n",
       "7   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2205.10511v1   \n",
       "8   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2005.11849v2   \n",
       "9   VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2009.01559v1   \n",
       "10  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2205.10455v2   \n",
       "11  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2101.04727v1   \n",
       "12  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2102.07624v1   \n",
       "13  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2103.02603v1   \n",
       "14  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  1711.07280v3   \n",
       "15  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     1208.6051   \n",
       "16  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     0802.2827   \n",
       "17  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     1102.1935   \n",
       "18  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2004.05343v1   \n",
       "19  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     0809.1489   \n",
       "20  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     0906.3896   \n",
       "21  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     1402.3314   \n",
       "22  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     1303.6242   \n",
       "23  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...    1508.01441   \n",
       "24  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     1402.2090   \n",
       "25  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  1905.10295v6   \n",
       "26  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     0705.1367   \n",
       "27  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2106.01223v1   \n",
       "28  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  1712.06113v3   \n",
       "29  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     0802.2108   \n",
       "30  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2004.04572v2   \n",
       "31  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...     1403.6303   \n",
       "32  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2108.03348v3   \n",
       "33  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2109.04066v1   \n",
       "\n",
       "                                           annotation  \\\n",
       "0                                       unanswerable\n",
       "   \n",
       "1                                       unanswerable\n",
       "   \n",
       "2   [Here are the extracted Tasks, Datasets, Metri...   \n",
       "3   [{'LEADERBOARD':{'Task': 'Math Word Problems',...   \n",
       "4   [ {'LEADERBOARD':{ 'Task': 'Keyword Spotting',...   \n",
       "5   [57.70, 53.77, 70.23, 69.17, 63.95, 56.37, 69....   \n",
       "6                                       unanswerable\n",
       "   \n",
       "7   [{'LEADERBOARD':{'Task': 'DocRE', 'Dataset': '...   \n",
       "8   [{'LEADERBOARD':{'Task': 'English GEC', 'Datas...   \n",
       "9   [{'LEADERBOARD':{'Task': 'Object Detection', '...   \n",
       "10  [{'LEADERBOARD':{'Model': '+ SSP', 'Tasks': ['...   \n",
       "11  [{'LEADERBOARD':{'Task': 'Textual Cloze Task',...   \n",
       "12  [Here are the extracted Tasks, Datasets, Metri...   \n",
       "13  [ {'LEADERBOARD':{ 'Task': 'ORE-(CC+EBUI)', 'D...   \n",
       "14  [{'LEADERBOARD':{'Task': 'Visually-Grounded Na...   \n",
       "15                                      unanswerable\n",
       "   \n",
       "16                                      unanswerable\n",
       "   \n",
       "17                                      unanswerable\n",
       "   \n",
       "18  [{'LEADERBOARD':{'Task': None, 'Dataset': ['Go...   \n",
       "19  [{'LEADERBOARD':{'Task': 'Facial Expression Re...   \n",
       "20  [{'LEADERBOARD':{'Task': 'Geometric Stabbing',...   \n",
       "21                                      unanswerable\n",
       "   \n",
       "22  [{'LEADERBOARD':{'Task': 'WSN simulation', 'Da...   \n",
       "23                                      unanswerable\n",
       "   \n",
       "24                                      unanswerable\n",
       "   \n",
       "25  [{'LEADERBOARD':{'Task': 'Few-shot learning', ...   \n",
       "26                                      unanswerable\n",
       "   \n",
       "27  [{'LEADERBOARD':{'Task': 'Named Entity Recogni...   \n",
       "28  [{'LEADERBOARD':{'Task': 'MD simulation', 'Dat...   \n",
       "29  [Here are the extracted Tasks, Datasets, Metri...   \n",
       "30  [Here are the extracted benchmark results:**Ta...   \n",
       "31                                      unanswerable\n",
       "   \n",
       "32  [Here are the extracted benchmark results:**Ta...   \n",
       "33  [{'LEADERBOARD':{'Task': 'Molweni', 'Dataset':...   \n",
       "\n",
       "                                         ground_truth  inference_s  \n",
       "0                                       unanswerable\n",
       "    21.783005  \n",
       "1                                       unanswerable\n",
       "     2.080140  \n",
       "2   [{'LEADERBOARD': {'Task': 'Text Summarization'...    27.891736  \n",
       "3   [{'LEADERBOARD': {'Task': 'Arithmetic Reasonin...    35.307533  \n",
       "4   [{'LEADERBOARD': {'Task': 'Keyword Spotting', ...    17.645482  \n",
       "5   [{'LEADERBOARD': {'Task': 'Aspect Sentiment Tr...    19.245811  \n",
       "6                                       unanswerable\n",
       "    12.290817  \n",
       "7   [{'LEADERBOARD': {'Task': 'Relation Extraction...    42.081739  \n",
       "8   [{'LEADERBOARD': {'Task': 'Grammatical Error C...    37.168867  \n",
       "9   [{'LEADERBOARD': {'Task': 'Instance Segmentati...    59.322940  \n",
       "10  [{'LEADERBOARD': {'Task': 'Question Answering'...    34.948370  \n",
       "11  [{'LEADERBOARD': {'Task': 'Question Answering'...    14.899040  \n",
       "12  [{'LEADERBOARD': {'Task': 'Action Spotting', '...    35.626044  \n",
       "13                                      unanswerable\n",
       "    41.838210  \n",
       "14  [{'LEADERBOARD': {'Task': 'Visual Navigation',...    26.552889  \n",
       "15                                      unanswerable\n",
       "    10.129432  \n",
       "16                                      unanswerable\n",
       "     7.930030  \n",
       "17                                      unanswerable\n",
       "     1.791833  \n",
       "18  [{'LEADERBOARD': {'Task': 'Deblurring', 'Datas...    11.181190  \n",
       "19                                      unanswerable\n",
       "    24.773965  \n",
       "20                                      unanswerable\n",
       "    25.420012  \n",
       "21                                      unanswerable\n",
       "     1.287452  \n",
       "22                                      unanswerable\n",
       "    24.049734  \n",
       "23                                      unanswerable\n",
       "     4.730810  \n",
       "24                                      unanswerable\n",
       "     2.543730  \n",
       "25  [{'LEADERBOARD': {'Task': 'Few-Shot Image Clas...    19.164492  \n",
       "26                                      unanswerable\n",
       "     4.582883  \n",
       "27  [{'LEADERBOARD': {'Task': 'Named Entity Recogn...    15.360946  \n",
       "28  [{'LEADERBOARD': {'Task': 'Formation Energy', ...    22.111865  \n",
       "29                                      unanswerable\n",
       "    17.489531  \n",
       "30  [{'LEADERBOARD': {'Task': '3D Object Reconstru...    19.956842  \n",
       "31                                      unanswerable\n",
       "    11.872271  \n",
       "32  [{'LEADERBOARD': {'Task': 'Link Prediction', '...    47.365411  \n",
       "33  [{'LEADERBOARD': {'Task': 'Question Answering'...    21.563660  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "runs = [\n",
    "    \"VAL_naive_doctaet-simple_zs-mistralaiMistral7BInstructv03-20240611-095123\",\n",
    "    \"VAL_naive_doctaet-simple_fs-mistralaiMistral7BInstructv03-20240611-102154\",\n",
    "    \"VAL_naive_doctaet-simple_zs-googlegemma7bit-20240611-130659\",\n",
    "    \"VAL_naive_doctaet-simple_fs-googlegemma7bit-20240611-133609\",\n",
    "    \"VAL_llama3_70b_fewshot_optimized02-20240531-105142\",\n",
    "    \"VAL_llama3_8b_few_shot_template_initial-20240604-125136\",\n",
    "    \"VAL_naive_doctaet-simple_zs-llama38b-20240611-212642\",\n",
    "    \"VAL_naive_doctaet-simple_fs-llama38b-20240611-213146\",\n",
    "    \"VAL_naive_doctaet-simple_zs-gemma7b-20240612-073229\",\n",
    "    \"VAL_naive_doctaet-simple_fs-gemma7b-20240612-073907\",\n",
    "    \"VAL_naive_doctaet-simple_zs-mistral7b-20240612-074427\",\n",
    "    \"VAL_naive_doctaet-simple_fs-mistral7b-20240612-075256\",\n",
    "    \"VAL_naive_doctaet-simple_zs-llama370b-20240612-083003\",\n",
    "    \"VAL_naive_doctaet-simple_fs-llama370b-20240612-085839\",\n",
    "    \"VAL_naive_doctaet-simple_fs_v2-llama370b-20240612-121227\",\n",
    "]\n",
    "\n",
    "# Top runs on Test:\n",
    "# - TEST_llama3_70b_fewshot_initial-20240601-211554\n",
    "dfs = []\n",
    "\n",
    "for run in runs:\n",
    "    path = f\"results/{run}/df.feather\"\n",
    "\n",
    "    df = pd.read_feather(path)\n",
    "    try:\n",
    "        df = df.rename({\"annontation\": \"annotation\"}, axis=1) # typo in early versions\n",
    "    except:\n",
    "        pass\n",
    "    dfs.append(df)\n",
    "from src.content_extraction import format\n",
    "\n",
    "# Some older runs might not be in the right format -> format function is idempotent\n",
    "for df in dfs:\n",
    "    df[\"annotation\"] = df[\"annotation\"].apply(format)\n",
    "    df[\"annotation\"] = df[\"annotation\"].astype(pd.StringDtype())\n",
    "    df[\"ground_truth\"] = df[\"ground_truth\"].astype(pd.StringDtype())\n",
    "dfs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run              object\n",
       "f                object\n",
       "annotation       string\n",
       "ground_truth     string\n",
       "inference_s     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[-1].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(preds)\n\u001b[0;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mMetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_property_wise_json_based\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     15\u001b[0m     Metrics\u001b[38;5;241m.\u001b[39mevaluate_rouge(label_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m]), prediction_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/code/nlp2024_ClefTask4SOTA/scoring_program/evaluation.py:1085\u001b[0m, in \u001b[0;36mMetrics.evaluate_property_wise_json_based\u001b[0;34m(label_list, prediction_list)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_property_wise_json_based\u001b[39m(label_list, prediction_list):\n\u001b[0;32m-> 1085\u001b[0m     evaluation_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_all_metrics_json_based\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m   1087\u001b[0m         k: \u001b[38;5;28mround\u001b[39m(v \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1088\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m flatten_result_dict(evaluation_dict)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1089\u001b[0m     }\n",
      "File \u001b[0;32m~/code/nlp2024_ClefTask4SOTA/scoring_program/evaluation.py:904\u001b[0m, in \u001b[0;36mcompute_all_metrics_json_based\u001b[0;34m(label_list, prediction_list)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_all_metrics_json_based\u001b[39m(label_list, prediction_list):\n\u001b[0;32m--> 904\u001b[0m     label_prediction_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mmake_list_of_pairs_json_based\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_list\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m     general_accuracy \u001b[38;5;241m=\u001b[39m Metrics\u001b[38;5;241m.\u001b[39mgeneral_accuracy_json_based(\n\u001b[1;32m    908\u001b[0m         label_list\u001b[38;5;241m=\u001b[39mlabel_list, prediction_list\u001b[38;5;241m=\u001b[39mprediction_list\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    910\u001b[0m     exact_recalls, partial_recalls \u001b[38;5;241m=\u001b[39m Metrics\u001b[38;5;241m.\u001b[39mall_recall_json_based(\n\u001b[1;32m    911\u001b[0m         label_list\u001b[38;5;241m=\u001b[39mlabel_list, pair_list\u001b[38;5;241m=\u001b[39mlabel_prediction_pairs\n\u001b[1;32m    912\u001b[0m     )\n",
      "File \u001b[0;32m~/code/nlp2024_ClefTask4SOTA/scoring_program/evaluation.py:487\u001b[0m, in \u001b[0;36mmake_list_of_pairs_json_based\u001b[0;34m(label_list, prediction_list)\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    483\u001b[0m             pair_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    484\u001b[0m                 (item1, item2, calculate_fuzz_ratio(item1_str, item2_str))\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m     top_similar_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mapply_hungarian_algorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpair_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlen_gt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel_contribution_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlen_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprediction_contribution_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     list_of_label_prediction_pairs\u001b[38;5;241m.\u001b[39mextend(top_similar_pairs)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m list_of_label_prediction_pairs\n",
      "File \u001b[0;32m~/code/nlp2024_ClefTask4SOTA/scoring_program/evaluation.py:409\u001b[0m, in \u001b[0;36mapply_hungarian_algorithm\u001b[0;34m(data_with_similarity_scores, len_gt, len_pred)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_gt):\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_pred):\n\u001b[0;32m--> 409\u001b[0m         similarity_matrix[i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_with_similarity_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# Convert to cost matrix for Hungarian algorithm\u001b[39;00m\n\u001b[1;32m    412\u001b[0m max_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Assuming 100 is the maximum possible similarity score\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "from scoring_program.evaluation import Metrics\n",
    "\n",
    "results = []\n",
    "for df in dfs[-1:]:\n",
    "    df = df[13:14]\n",
    "    labels = list(df[\"ground_truth\"])\n",
    "    preds = list(df[\"annotation\"])\n",
    "    assert len(labels) == len(preds)\n",
    "    result = Metrics.evaluate_property_wise_json_based(\n",
    "        label_list=labels, prediction_list=preds\n",
    "    )\n",
    "\n",
    "    result.update(\n",
    "        Metrics.evaluate_rouge(label_list=list(df[\"ground_truth\"]), prediction_list=list(df[\"annotation\"]))\n",
    "    )\n",
    "    result[\"run\"] = df[\"run\"].iloc[0]\n",
    "    result[\"score\"] = result[\"partial_f1s_overall\"]\n",
    "    result[\"num_samples\"] = len(df)\n",
    "    result[\"mean_t\"] = df[\"inference_s\"].mean()\n",
    "    results.append(pd.DataFrame([result]))\n",
    "\n",
    "df_res = pd.concat(results)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().to_feather(\"troubleshoot.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>f</th>\n",
       "      <th>annotation</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>inference_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>VAL_naive_doctaet-simple_fs_v2-llama370b-20240...</td>\n",
       "      <td>2108.03348v3</td>\n",
       "      <td>[Here are the extracted benchmark results:**Ta...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Link Prediction', '...</td>\n",
       "      <td>47.365411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  run             f  \\\n",
       "32  VAL_naive_doctaet-simple_fs_v2-llama370b-20240...  2108.03348v3   \n",
       "\n",
       "                                           annotation  \\\n",
       "32  [Here are the extracted benchmark results:**Ta...   \n",
       "\n",
       "                                         ground_truth  inference_s  \n",
       "32  [{'LEADERBOARD': {'Task': 'Link Prediction', '...    47.365411  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea09856634a4cf08d21f0d97e13db5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "To be able to use evaluate-metric/rouge, you need to install the following dependencies['nltk', 'rouge_score'] using 'pip install # Here to have a nice missing dependency error message early on rouge_score' for instance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(preds)\n\u001b[0;32m      9\u001b[0m result \u001b[38;5;241m=\u001b[39m Metrics\u001b[38;5;241m.\u001b[39mevaluate_property_wise_json_based(\n\u001b[0;32m     10\u001b[0m     label_list\u001b[38;5;241m=\u001b[39mlabels, prediction_list\u001b[38;5;241m=\u001b[39mpreds\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mMetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_rouge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mground_truth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\urban\\OneDrive\\HSG\\S2\\Natural Language Processing\\group_assignment\\Project\\nlp2024_ClefTask4SOTA\\scoring_program\\evaluation.py:1093\u001b[0m, in \u001b[0;36mMetrics.evaluate_rouge\u001b[1;34m(label_list, prediction_list)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_rouge\u001b[39m(label_list, prediction_list):\n\u001b[1;32m-> 1093\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrouge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m     result \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m   1095\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mprediction_list, references\u001b[38;5;241m=\u001b[39mlabel_list, use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m     )\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mround\u001b[39m(v \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\urban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\evaluate\\loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[1;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[0;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[0;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[0;32m    761\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\urban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\evaluate\\loading.py:680\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[1;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (\u001b[38;5;167;01mConnectionError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m)):\n\u001b[1;32m--> 680\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    682\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    683\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hugging Face Hub either.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    684\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\urban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\evaluate\\loading.py:639\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[1;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasurement\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubEvaluationModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluate-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 639\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\urban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\evaluate\\loading.py:489\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    488\u001b[0m imports \u001b[38;5;241m=\u001b[39m get_imports(local_path)\n\u001b[1;32m--> 489\u001b[0m local_imports \u001b[38;5;241m=\u001b[39m \u001b[43m_download_additional_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_hub_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# copy the script and the files in an importable directory\u001b[39;00m\n\u001b[0;32m    496\u001b[0m dynamic_modules_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;28;01melse\u001b[39;00m init_dynamic_modules()\n",
      "File \u001b[1;32mc:\\Users\\urban\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\evaluate\\loading.py:265\u001b[0m, in \u001b[0;36m_download_additional_modules\u001b[1;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[0;32m    263\u001b[0m         needs_to_be_installed\u001b[38;5;241m.\u001b[39madd((library_import_name, library_import_path))\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_to_be_installed:\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be able to use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, you need to install the following dependencies\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[lib_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlib_name,\u001b[38;5;250m \u001b[39mlib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mneeds_to_be_installed]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlib_name,\u001b[38;5;250m \u001b[39mlib_path\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mneeds_to_be_installed])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for instance\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_imports\n",
      "\u001b[1;31mImportError\u001b[0m: To be able to use evaluate-metric/rouge, you need to install the following dependencies['nltk', 'rouge_score'] using 'pip install # Here to have a nice missing dependency error message early on rouge_score' for instance'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scoring_program.evaluation import Metrics\n",
    "from src.content_extraction import format\n",
    "df = pd.read_feather(\"troubleshoot.feather\")\n",
    "df[\"annotation\"] = df[\"annotation\"].apply(format)\n",
    "labels = list(df[\"ground_truth\"])\n",
    "preds = list(df[\"annotation\"])\n",
    "assert len(labels) == len(preds)\n",
    "result = Metrics.evaluate_property_wise_json_based(\n",
    "    label_list=labels, prediction_list=preds\n",
    ")\n",
    "\n",
    "result.update(\n",
    "    Metrics.evaluate_rouge(label_list=list(df[\"ground_truth\"]), prediction_list=list(df[\"annotation\"]))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
