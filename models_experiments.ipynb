{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 09:50:55.146088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.getcwd() == '/home/user/code':\n",
    "    os.chdir('/home/user/code/nlp2024_ClefTask4SOTA')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import transformers\n",
    "transformers.set_seed(17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What should be the max length of the model?\n",
    "\n",
    "Approach: Tokenize the longest annotation in the dataset, this is the max lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import TDMSDataset, PATH\n",
    "\n",
    "\n",
    "# load all annotations:\n",
    "dataset = TDMSDataset(PATH.TRAIN)\n",
    "\n",
    "annotations = []\n",
    "for i in range(len(dataset)):\n",
    "    f, tex, jsn = dataset[i]\n",
    "    annotations.append(str(jsn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame(annotations)\n",
    "df.columns = [\"annotation\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"str_len\"] = df.apply(lambda row: len(row[\"annotation\"]), axis=1)\n",
    "\n",
    "# top 10 longest annotations by string lenght:\n",
    "df_t10 = df.sort_values(by=\"str_len\", ascending=False)[:10]\n",
    "df_t10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring the outlier of 56561 chars and all below 20k chars as candidates for longest annotation\n",
    "\n",
    "df_longest = df_t10[1:6]\n",
    "df_longest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the longest annotation character wise also the longest annotation token wise?\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "df_longest[\"token_len\"] = df_longest.apply(lambda row: len(tokenizer(row[\"annotation\"])[\"input_ids\"]), axis=1)\n",
    "df_longest[\"token_len\"].max()\n",
    "\n",
    "# longest = 9878 tokens => Max len should be 10k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Other Tokenizers (llama)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "df_longest[\"token_len\"] = df_longest.apply(lambda row: len(tokenizer(row[\"annotation\"])[\"input_ids\"]), axis=1)\n",
    "df_longest[\"token_len\"].max()\n",
    "\n",
    "# longest = 8670 tokens => Max len should be 8700 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Other Tokenizers (gemma)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "\n",
    "df_longest[\"token_len\"] = df_longest.apply(lambda row: len(tokenizer(row[\"annotation\"])[\"input_ids\"]), axis=1)\n",
    "df_longest[\"token_len\"].max()\n",
    "\n",
    "# longest = 8898 tokens => Max len should be 9k tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3 8b vs Mistral 7b vs Gemma 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006412982940673828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 21,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a732b8fdf040e993160571fe358434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0070536136627197266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 21,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd222f0673b40348d4cd599920608fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.models import Model\n",
    "\n",
    "\n",
    "mistral_it = Model(\"mistralai/Mistral-7B-Instruct-v0.3\", 0)\n",
    "gemma_it = Model(\"google/gemma-7b-it\", 1)\n",
    "# llama_it = HFModel(\"meta-llama/Meta-Llama-3-8B-Instruct\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTuna.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_it.generate(\"Give an example of a kind of fish, reply be stating the name only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"\"\"[!tp]\\setlength{\\tabcolsep}{0.5pt}\n",
    "\t\\begin{center}\n",
    "\t\t\\caption{Performance comparison on Oulu-CASIA database in terms of average classification accuracy of the 10-fold cross-validation when evaluating on three different test sets, including ``weak expression\", ``peak expression\" and ``combined\", respectively.}\n",
    "\t\t\\label{table:oulu_compare}\n",
    "\t\t\\begin{tabular}{c|c|c|c}\n",
    "\t\t\t\\hline\\noalign{\\smallskip}\n",
    "\t\t\tMethod & weak expression & peak expression & combined\\\\\n",
    "\t\t\t\\hline\n",
    "\t\t\tPPDN(standard SGD) &  67.05\\% & 82.91\\% &73.54\\%\\\\\t\n",
    "\t\t\tGoogLeNet (baseline) & 64.64\\%& 79.21\\% &71.32\\%\\\\\n",
    "\t\t\t\\hline\n",
    "\t\t\tPPDN  & \\textbf{67.95\\%}&\\textbf{84.59\\%} & \\textbf{74.99\\%}\\\\\n",
    "\t\t\t\\hline\n",
    "\t\t\\end{tabular}\n",
    "\t\\end{center}\"\"\"\n",
    "\n",
    "test_text2 = \"\"\"Once the features were collected, they were shown to three separate individuals not present in the first experiment. They were shown the picture and asked to answer whether the given list of features found by the participants in the first experiment were present in the picture or not. Their responses are shown in the column labeled \"Freq and Res\" where a \\tick represents that the corresponding participant believed the feature to be present in the picture. Not surprisingly, the features with the higher frequencies were answered correctly by all three users. On the other hand, some of the single frequency features were also answered correctly by all three users. The ones with indifferent answers are those that require a `keen' eye, e.g. ``hook''.\n",
    "These experiments show that even a simple picture as the one shown in Figure 1 can have a lot of features, majority of which are very easy to answer but not so easy to extract. There can still be a lot more features present in the picture; one such example is \"`one side of a rubik's cube\"'.\n",
    "This survey gives us some guidelines while choosing the pictures and/or secret questions:\n",
    "\\begin{itemize}\n",
    "\t\\item Do not use pictures whose main object is the secret feature. So for example, if we chose the picture of Figure 1 as the challenge picture, then the secret question of \"`Does the picture contain the digits 1-9?\"' will certainly be a bad choice.\n",
    "\t\\item Do not use simple pictures. Simple pictures contain very few features. This is evident from Figure 1. Although, one may still be able to think of more features, there does not seem to be a big number of features.\n",
    "\t\\item Do not use secret questions which are hard to answer by the legitimate user. As an example, the feature \"`Equilibrium\"' in the above table was answered \"`no\"' by two users. This seems hard to find out in the picture and needs more of a philosophical eye. \n",
    "\t\\item Always allow for user error. So for example, if the user replies '10' pictures, allow an error of 2 to 3 wrong answers. This is clear from the picture that a user answered \"`no\"' to the feature \"`Mathematics\"', even though it seems to be describe the figure. \n",
    "\\end{itemize}\"\"\"\n",
    "mistral_it.generate(simple_fs(test_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 100/100 [30:30<00:00, 18.30s/it]\n",
      " 94%|█████████████████████████████████████▌  | 94/100 [1:39:25<10:39, 106.55s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:30001/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "from src.experiment_runner import Experiment, run\n",
    "from src.dataset import PATH\n",
    "from src.prompt_templates import simple_zs, simple_fs\n",
    "from src.content_extraction import naive_doctaet, empty_to_unanswerable\n",
    "\n",
    "\n",
    "exps = [\n",
    "    # Experiment(mistral_it, simple_zs, naive_doctaet, empty_to_unanswerable),\n",
    "    # Experiment(mistral_it, simple_fs, naive_doctaet, empty_to_unanswerable),\n",
    "    Experiment(gemma_it, simple_zs, naive_doctaet, empty_to_unanswerable),\n",
    "    Experiment(gemma_it, simple_fs, naive_doctaet, empty_to_unanswerable),\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for exp in exps:\n",
    "    # df = run(exp, PATH.VAL, max_iter=10)\n",
    "    df = run(exp, PATH.VAL)\n",
    "    dfs.append(df)\n",
    "\n",
    "# for exp in exps:\n",
    "#     df = run(exp, PATH.TEST)\n",
    "#     dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"VAL_mistral-it_fs_initial-20240610-221117\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_feather(f\"results/{run}/df.feather\")\n",
    "\n",
    "for annotation in df[\"annotation\"][:10]:\n",
    "    print(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiment_runner import free_cuda_memory\n",
    "\n",
    "free_cuda_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
