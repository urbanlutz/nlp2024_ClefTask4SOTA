{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshoot models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd() == '/home/user/code':\n",
    "    os.chdir('/home/user/code/nlp2024_ClefTask4SOTA')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_fs_v2(tex):\n",
    "    return f\"\"\"If the text reports benchmark results, extract the reported Tasks, Datasets, Metrics and Scores.\n",
    "\n",
    "Each benchmark result is represented by an object with four attributes: Task, Dataset, Metric, Score.\n",
    "    \n",
    "The format is as follows:\n",
    "[\n",
    "    {{\"Task\": \"example Task 1\", \"Dataset\": \"example Dataset 1\", \"Metric\": example metric 1\", \"Score\": \"score\"}}, \n",
    "    {{\"Task\": \"example Task 1\",\"Dataset\": \"example Dataset 2\", \"Metric\": example metric 2\", \"Score\": \"score\"}}\n",
    "]\n",
    "\n",
    "Heres an example:\n",
    "Text: \n",
    "table\n",
    "[!tp]\\\\setlength{{\\\\tabcolsep}}{{0.5pt}}\n",
    "\\\\begin{{center}}\n",
    "    \\\\caption{{Performance comparison on Oulu-CASIA database in terms of average classification accuracy of the 10-fold cross-validation when evaluating on three different test sets, including ``weak expression\", ``peak expression\" and ``combined\", respectively.}}\n",
    "    \\\\label{{table:oulu_compare}}\n",
    "    \\\\begin{{tabular}}{{c|c|c|c}}\n",
    "        \\\\hline\\\\noalign{{\\\\smallskip}}\n",
    "        Method & weak expression & peak expression & combined\\\\\\\\\n",
    "        \\\\hline\n",
    "        PPDN(standard SGD) &  67.05\\\\% & 82.91\\\\% &73.54\\\\%\\\\\\\\\t\n",
    "        GoogLeNet (baseline) & 64.64\\\\%& 79.21\\\\% &71.32\\\\%\\\\\\\\\n",
    "        \\\\hline\n",
    "        PPDN  & \\\\textbf{{67.95\\\\%}}&\\\\textbf{{84.59\\\\%}} & \\\\textbf{{74.99\\\\%}}\\\\\\\\\n",
    "        \\\\hline\n",
    "    \\\\end{{tabular}}\n",
    "\\\\end{{center}}  and provide the JSON Array only.\n",
    "\n",
    "Provide a JSON Array of objects in the specified format above. If no entry is found, return an empty JSON Array.\n",
    "\n",
    "\n",
    "Entries:\n",
    "[\n",
    "    {{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}\n",
    "]\n",
    "\n",
    "Text:\n",
    "{tex}\n",
    "\n",
    "Provide a JSON Array of objects in the specified format above. If no entry is found, return an empty JSON Array.\n",
    "\n",
    "Entries:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the extracted information:\n",
      "\n",
      "**Tasks**\n",
      "\n",
      "* Evaluation of throughput for different coding schemes (unrestricted, full-cache, source-only, no-coding)\n",
      "* Comparison of static and mobile scenarios\n",
      "\n",
      "**Datasets**\n",
      "\n",
      "* Corridor model with 30% packet loss\n",
      "* Mobile scenario with 10 nodes, 3 publishers, and 7 receivers\n",
      "\n",
      "**Metrics**\n",
      "\n",
      "* Throughput\n",
      "\n",
      "**Results**\n",
      "\n",
      "* Figures ~\\ref{fig:static} and ~\\ref{fig:mobile} show the results for static and mobile scenarios respectively\n",
      "* Full-cache coding competes with unrestricted coding in both scenarios\n",
      "* Source-only coding delivers files quicker than no-coding, but not as efficiently as full-cache or unrestricted coding\n",
      "\n",
      "Let me know if you'd like me to help with anything else!\n",
      "v2-----------------------------\n",
      "Here are the extracted tasks, datasets, metrics, and scores:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"Task\": null,\n",
      "    \"Dataset\": null,\n",
      "    \"Metric\": null,\n",
      "    \"Score\": null\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that there is no specific task, dataset, metric, or score mentioned in the text. The evaluation section only discusses the comparison of different coding techniques (full cache coding, unrestricted coding, source-only coding, and no coding) using two scenarios (corridor model with packet loss and mobile model with mobility).\n",
      "formated-----------------------\n",
      "[{'LEADERBOARD': {'Task': None, 'Dataset': None, 'Metric': None, 'Score': None}}]\n",
      "truth--------------------------\n",
      "unanswerable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import OllamaModel, ARCHITECTURE\n",
    "from src.dataset import TDMSDataset, PATH\n",
    "from src.prompt_templates import simple_fs\n",
    "\n",
    "model = OllamaModel(ARCHITECTURE.LLAMA_8b)\n",
    "\n",
    "dataset = TDMSDataset(PATH.VAL)\n",
    "\n",
    "f, tex, jsn = dataset[99]\n",
    "\n",
    "prompt = simple_fs(tex)\n",
    "response = model.generate(prompt)\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "prompt = simple_fs_v2(tex)\n",
    "\n",
    "response = model.generate(prompt)\n",
    "\n",
    "print(\"v2-----------------------------\")\n",
    "print(response)\n",
    "\n",
    "print(\"formated-----------------------\")\n",
    "print(format(response))\n",
    "print(\"truth--------------------------\")\n",
    "print(jsn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'LEADERBOARD': {'Task': '+ PSD Data (MLM-only)', 'Dataset': None, 'Metric': None, 'Score': [{'name': 'Tasks', 'value': 'Benchmark Results'}, {'name': 'Datasets', 'value': ['WikiQA', 'TREC-QA']}, {'name': 'Metrics', 'value': ['Accuracy', 'F1 Score']}, {'name': 'Scores', 'value': [0.85, 0.92]}]}}, {'LEADERBOARD': {'Task': '+ WikiQA', 'Dataset': 'WikiQA', 'Metric': 'Accuracy', 'Score': 0.85}}, {'LEADERBOARD': {'Task': '+ TREC-QA', 'Dataset': 'TREC-QA', 'Metric': 'F1 Score', 'Score': 0.92}}]\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.content_extraction import format \n",
    "\n",
    "import re\n",
    "\n",
    "text = format(response)\n",
    "\n",
    "# def add_LEADERBOARD(text):\n",
    "#     text = text.replace( \"{LEADERBOARD:{\", \"{\").replace(\"}}\", \"}\") # remove potential leaderboards for idempotency\n",
    "#     return text.replace(\"{\", \"{LEADERBOARD:{\").replace(\"}\", \"}}\")\n",
    "\n",
    "# add_LEADERBOARD(text) == add_LEADERBOARD(add_LEADERBOARD(text))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how much information is in there?\n",
    "import json\n",
    "text = json.dumps([{'LEADERBOARD': {'Task': None, 'Dataset': None, 'Metric': None, 'Score': None}}])\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "text = \"[{'LEADERBOARD': {'Task': '+ PSD Data (MLM-only)', 'Dataset': None, 'Metric': None, 'Score': [{'name': 'Tasks', 'value': 'Benchmark Results'}, {'name': 'Datasets', 'value': ['WikiQA', 'TREC-QA']}, {'name': 'Metrics', 'value': ['Accuracy', 'F1 Score']}, {'name': 'Scores', 'value': [0.85, 0.92]}]}}, {'LEADERBOARD': {'Task': '+ WikiQA', 'Dataset': 'WikiQA', 'Metric': 'Accuracy', 'Score': 0.85}}, {'LEADERBOARD': {'Task': '+ TREC-QA', 'Dataset': 'TREC-QA', 'Metric': 'F1 Score', 'Score': 0.92}}]\"\n",
    "\n",
    "information(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f'[{{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"LEADERBOARD\": {\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '[{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}]'\n",
    "\n",
    "\n",
    "add_LEADERBOARD(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{LEADERBOARD:{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"LEADERBOARD\": {\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
