{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshoot models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd() == '/home/user/code':\n",
    "    os.chdir('/home/user/code/nlp2024_ClefTask4SOTA')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_fs_v2(tex):\n",
    "    return f\"\"\"If the text reports benchmark results, extract the reported Tasks, Datasets, Metrics and Scores.\n",
    "\n",
    "Each benchmark result is represented by an object with four attributes: Task, Dataset, Metric, Score.\n",
    "    \n",
    "The format is as follows:\n",
    "[\n",
    "    {{\"Task\": \"example Task 1\", \"Dataset\": \"example Dataset 1\", \"Metric\": example metric 1\", \"Score\": \"score\"}}, \n",
    "    {{\"Task\": \"example Task 1\",\"Dataset\": \"example Dataset 2\", \"Metric\": example metric 2\", \"Score\": \"score\"}}\n",
    "]\n",
    "\n",
    "Heres an example:\n",
    "Text: \n",
    "table\n",
    "[!tp]\\\\setlength{{\\\\tabcolsep}}{{0.5pt}}\n",
    "\\\\begin{{center}}\n",
    "    \\\\caption{{Performance comparison on Oulu-CASIA database in terms of average classification accuracy of the 10-fold cross-validation when evaluating on three different test sets, including ``weak expression\", ``peak expression\" and ``combined\", respectively.}}\n",
    "    \\\\label{{table:oulu_compare}}\n",
    "    \\\\begin{{tabular}}{{c|c|c|c}}\n",
    "        \\\\hline\\\\noalign{{\\\\smallskip}}\n",
    "        Method & weak expression & peak expression & combined\\\\\\\\\n",
    "        \\\\hline\n",
    "        PPDN(standard SGD) &  67.05\\\\% & 82.91\\\\% &73.54\\\\%\\\\\\\\\t\n",
    "        GoogLeNet (baseline) & 64.64\\\\%& 79.21\\\\% &71.32\\\\%\\\\\\\\\n",
    "        \\\\hline\n",
    "        PPDN  & \\\\textbf{{67.95\\\\%}}&\\\\textbf{{84.59\\\\%}} & \\\\textbf{{74.99\\\\%}}\\\\\\\\\n",
    "        \\\\hline\n",
    "    \\\\end{{tabular}}\n",
    "\\\\end{{center}}  and provide the JSON Array only.\n",
    "\n",
    "Provide a JSON Array of objects in the specified format above. If no entry is found, return an empty JSON Array.\n",
    "\n",
    "\n",
    "Entries:\n",
    "[\n",
    "    {{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}\n",
    "]\n",
    "\n",
    "Text:\n",
    "{tex}\n",
    "\n",
    "Provide a JSON Array of objects in the specified format above. If no entry is found, return an empty JSON Array.\n",
    "\n",
    "Entries:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the extracted information:\n",
      "\n",
      "**Tasks**: None mentioned explicitly, but based on the table, it appears to be a question answering task.\n",
      "\n",
      "**Datasets**: ASNQ, WikiQA, TREC-QA (AS2 datasets)\n",
      "\n",
      "**Metrics**: Not explicitly mentioned, but likely includes metrics such as accuracy or F1-score.\n",
      "v2--------------------------\n",
      "Based on the provided table, I extracted the reported Tasks, Datasets, Metrics and Scores. Here are the results:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"Task\": \"+ PSD Data (MLM-only)\",\n",
      "    \"Dataset\": null,\n",
      "    \"Metric\": null,\n",
      "    \"Score\": [\n",
      "      {\"name\": \"Tasks\", \"value\": \"Benchmark Results\"},\n",
      "      {\"name\": \"Datasets\", \"value\": [\"WikiQA\", \"TREC-QA\"]},\n",
      "      {\"name\": \"Metrics\", \"value\": [\"Accuracy\", \"F1 Score\"]},\n",
      "      {\"name\": \"Scores\", \"value\": [0.85, 0.92]}\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"Task\": \"+ WikiQA\",\n",
      "    \"Dataset\": \"WikiQA\",\n",
      "    \"Metric\": \"Accuracy\",\n",
      "    \"Score\": 0.85\n",
      "  },\n",
      "  {\n",
      "    \"Task\": \"+ TREC-QA\",\n",
      "    \"Dataset\": \"TREC-QA\",\n",
      "    \"Metric\": \"F1 Score\",\n",
      "    \"Score\": 0.92\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that the scores are not explicitly reported for each task and dataset, so I only included the reported scores in the JSON array. If you need more information or clarification on any of these points, please let me know!\n",
      "truth--------------------------\n",
      "[{'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TrecQA', 'Metric': 'MAP', 'Score': '0.923'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TrecQA', 'Metric': 'MRR', 'Score': '0.946'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TrecQA', 'Metric': 'MAP', 'Score': '0.903'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TrecQA', 'Metric': 'MRR', 'Score': '0.951'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WikiQA', 'Metric': 'MAP', 'Score': '0.909'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WikiQA', 'Metric': 'MRR', 'Score': '0.920'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WikiQA', 'Metric': 'MAP', 'Score': '0.901'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WikiQA', 'Metric': 'MRR', 'Score': '0.914'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WikiQA', 'Metric': 'MAP', 'Score': '0.887'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WikiQA', 'Metric': 'MRR', 'Score': '0.899'}}, {'LEADERBOARD': {'Task': 'Answer Selection', 'Dataset': 'ASNQ', 'Metric': 'MAP', 'Score': '0.743'}}, {'LEADERBOARD': {'Task': 'Answer Selection', 'Dataset': 'ASNQ', 'Metric': 'MRR', 'Score': '0.800'}}, {'LEADERBOARD': {'Task': 'Answer Selection', 'Dataset': 'ASNQ', 'Metric': 'MAP', 'Score': '0.697'}}, {'LEADERBOARD': {'Task': 'Answer Selection', 'Dataset': 'ASNQ', 'Metric': 'MRR', 'Score': '0.757'}}]\n"
     ]
    }
   ],
   "source": [
    "from src.models import OllamaModel, ARCHITECTURE\n",
    "from src.dataset import TDMSDataset, PATH\n",
    "from src.prompt_templates import simple_fs\n",
    "\n",
    "model = OllamaModel(ARCHITECTURE.LLAMA_8b)\n",
    "\n",
    "dataset = TDMSDataset(PATH.VAL)\n",
    "\n",
    "f, tex, jsn = dataset[10]\n",
    "\n",
    "prompt = simple_fs(tex)\n",
    "response = model.generate(prompt)\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "prompt = simple_fs_v2(tex)\n",
    "\n",
    "response = model.generate(prompt)\n",
    "\n",
    "print(\"v2--------------------------\")\n",
    "print(response)\n",
    "\n",
    "print(\"truth--------------------------\")\n",
    "print(jsn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.content_extraction import format \n",
    "\n",
    "import re\n",
    "\n",
    "text = format(response)\n",
    "\n",
    "# def add_LEADERBOARD(text):\n",
    "#     text = text.replace( \"{LEADERBOARD:{\", \"{\").replace(\"}}\", \"}\") # remove potential leaderboards for idempotency\n",
    "#     return text.replace(\"{\", \"{LEADERBOARD:{\").replace(\"}\", \"}}\")\n",
    "\n",
    "# add_LEADERBOARD(text) == add_LEADERBOARD(add_LEADERBOARD(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f'[{{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"LEADERBOARD\": {\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '[{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}]'\n",
    "\n",
    "\n",
    "add_LEADERBOARD(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{LEADERBOARD:{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"LEADERBOARD\": {\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
