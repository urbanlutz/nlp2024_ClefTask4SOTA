{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshoot models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.getcwd() == '/home/user/code':\n",
    "    os.chdir('/home/user/code/nlp2024_ClefTask4SOTA')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_fs_v2(tex):\n",
    "    return f\"\"\"If the text reports benchmark results, extract the reported Tasks, Datasets, Metrics and Scores.\n",
    "\n",
    "Each benchmark result is represented by an object with four attributes: Task, Dataset, Metric, Score.\n",
    "    \n",
    "The format is as follows:\n",
    "[\n",
    "    {{\"Task\": \"example Task 1\", \"Dataset\": \"example Dataset 1\", \"Metric\": example metric 1\", \"Score\": \"score\"}}, \n",
    "    {{\"Task\": \"example Task 1\",\"Dataset\": \"example Dataset 2\", \"Metric\": example metric 2\", \"Score\": \"score\"}}\n",
    "]\n",
    "\n",
    "Heres an example:\n",
    "Text: \n",
    "table\n",
    "[!tp]\\\\setlength{{\\\\tabcolsep}}{{0.5pt}}\n",
    "\\\\begin{{center}}\n",
    "    \\\\caption{{Performance comparison on Oulu-CASIA database in terms of average classification accuracy of the 10-fold cross-validation when evaluating on three different test sets, including ``weak expression\", ``peak expression\" and ``combined\", respectively.}}\n",
    "    \\\\label{{table:oulu_compare}}\n",
    "    \\\\begin{{tabular}}{{c|c|c|c}}\n",
    "        \\\\hline\\\\noalign{{\\\\smallskip}}\n",
    "        Method & weak expression & peak expression & combined\\\\\\\\\n",
    "        \\\\hline\n",
    "        PPDN(standard SGD) &  67.05\\\\% & 82.91\\\\% &73.54\\\\%\\\\\\\\\t\n",
    "        GoogLeNet (baseline) & 64.64\\\\%& 79.21\\\\% &71.32\\\\%\\\\\\\\\n",
    "        \\\\hline\n",
    "        PPDN  & \\\\textbf{{67.95\\\\%}}&\\\\textbf{{84.59\\\\%}} & \\\\textbf{{74.99\\\\%}}\\\\\\\\\n",
    "        \\\\hline\n",
    "    \\\\end{{tabular}}\n",
    "\\\\end{{center}}  and provide the JSON Array only.\n",
    "\n",
    "Provide a JSON Array of objects in the specified format above. If no entry is found, return an empty JSON Array.\n",
    "\n",
    "\n",
    "Entries:\n",
    "[\n",
    "    {{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}\n",
    "]\n",
    "\n",
    "Text:\n",
    "{tex}\n",
    "\n",
    "Provide a JSON Array of objects in the specified format above. If no entry is found, return an empty JSON Array.\n",
    "\n",
    "Entries:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the JSON array:\n",
      "\n",
      "[\n",
      "  {LEADERBOARD: {\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu- CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}},\n",
      "  {LEADERBOARD: {\"Task\": \"\", \"Dataset\": \"\", \"Metric\": \"\", \"Score\": \"\"}}\n",
      "]\n",
      "v2-----------------------------\n",
      "Here are the extracted entries in the specified format:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-ASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}\n",
      "]\n",
      "```\n",
      "formated-----------------------\n",
      "[{'LEADERBOARD': {'Task': 'Facial Expression Recognition (FER)', 'Dataset': 'Oulu-ASIA', 'Metric': 'Accuracy (10-fold)', 'Score': '84.59'}}]\n",
      "truth--------------------------\n",
      "unanswerable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import OllamaModel, ARCHITECTURE\n",
    "from src.dataset import TDMSDataset, PATH\n",
    "from src.prompt_templates import simple_fs\n",
    "\n",
    "model = OllamaModel(ARCHITECTURE.LLAMA_8b)\n",
    "\n",
    "dataset = TDMSDataset(PATH.VAL)\n",
    "\n",
    "f, tex, jsn = dataset[99]\n",
    "\n",
    "prompt = simple_fs(tex)\n",
    "response = model.generate(prompt)\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "prompt = simple_fs_v2(tex)\n",
    "\n",
    "response = model.generate(prompt)\n",
    "\n",
    "print(\"v2-----------------------------\")\n",
    "print(response)\n",
    "\n",
    "print(\"formated-----------------------\")\n",
    "print(format(response))\n",
    "print(\"truth--------------------------\")\n",
    "print(jsn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ctx_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'LEADERBOARD': {'Task': '+ PSD Data (MLM-only)', 'Dataset': None, 'Metric': None, 'Score': [{'name': 'Tasks', 'value': 'Benchmark Results'}, {'name': 'Datasets', 'value': ['WikiQA', 'TREC-QA']}, {'name': 'Metrics', 'value': ['Accuracy', 'F1 Score']}, {'name': 'Scores', 'value': [0.85, 0.92]}]}}, {'LEADERBOARD': {'Task': '+ WikiQA', 'Dataset': 'WikiQA', 'Metric': 'Accuracy', 'Score': 0.85}}, {'LEADERBOARD': {'Task': '+ TREC-QA', 'Dataset': 'TREC-QA', 'Metric': 'F1 Score', 'Score': 0.92}}]\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.content_extraction import format \n",
    "\n",
    "import re\n",
    "\n",
    "text = format(response)\n",
    "\n",
    "# def add_LEADERBOARD(text):\n",
    "#     text = text.replace( \"{LEADERBOARD:{\", \"{\").replace(\"}}\", \"}\") # remove potential leaderboards for idempotency\n",
    "#     return text.replace(\"{\", \"{LEADERBOARD:{\").replace(\"}\", \"}}\")\n",
    "\n",
    "# add_LEADERBOARD(text) == add_LEADERBOARD(add_LEADERBOARD(text))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how much information is in there?\n",
    "import json\n",
    "text = json.dumps([{'LEADERBOARD': {'Task': None, 'Dataset': None, 'Metric': None, 'Score': None}}])\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "text = \"[{'LEADERBOARD': {'Task': '+ PSD Data (MLM-only)', 'Dataset': None, 'Metric': None, 'Score': [{'name': 'Tasks', 'value': 'Benchmark Results'}, {'name': 'Datasets', 'value': ['WikiQA', 'TREC-QA']}, {'name': 'Metrics', 'value': ['Accuracy', 'F1 Score']}, {'name': 'Scores', 'value': [0.85, 0.92]}]}}, {'LEADERBOARD': {'Task': '+ WikiQA', 'Dataset': 'WikiQA', 'Metric': 'Accuracy', 'Score': 0.85}}, {'LEADERBOARD': {'Task': '+ TREC-QA', 'Dataset': 'TREC-QA', 'Metric': 'F1 Score', 'Score': 0.92}}]\"\n",
    "\n",
    "information(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f'[{{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"LEADERBOARD\": {\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '[{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}]'\n",
    "\n",
    "\n",
    "add_LEADERBOARD(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{LEADERBOARD:{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"LEADERBOARD\": {\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}}]'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"Task\": \"Facial Expression Recognition (FER)\", \"Dataset\": \"Oulu-CASIA\", \"Metric\": \"Accuracy (10-fold)\", \"Score\": \"84.59\"}]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
