{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd() == '/home/user/code':\n",
    "    os.chdir('/home/user/code/nlp2024_ClefTask4SOTA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TexSoup import TexSoup\n",
    "import re\n",
    "\n",
    "def find_sections(tex):\n",
    "    return [match.group(1) for match in re.finditer(\"\\\\\\section\\{(.*)\\}\", tex)]\n",
    "\n",
    "\n",
    "def extract_content(node, name):\n",
    "    if node:\n",
    "        return ((name, node.contents[0]))\n",
    "\n",
    "\n",
    "def section_split(tex):\n",
    "    \"\"\"returns: List of tuples (section_name, section_text)\"\"\"\n",
    "    doc_text = tex\n",
    "    sections = []\n",
    "    # parsing with TexSoup\n",
    "    # TODO: get rid of this and use regex only (brittle)\n",
    "    try:\n",
    "        soup = TexSoup(tex, tolerance=1)\n",
    "        if title:=extract_content(soup.title, \"title\"):\n",
    "            sections.append(title)\n",
    "        if abstract:=extract_content(soup.abstract, \"abstract\"):\n",
    "            sections.append(abstract)\n",
    "        sections.append((\"tables\", \"\\n\".join([str(node) for node in soup.find_all(\"table\")]))) if soup.find_all(\"table\") else None\n",
    "    except:\n",
    "        pass # could not parse tex\n",
    "\n",
    "    # extract latex sections and corresponding text\n",
    "    prev_section = \"pre\"\n",
    "\n",
    "    for section in find_sections(tex):\n",
    "        section_text, doc_text = doc_text.split(f\"\\\\section{{{section}}}\", 1)\n",
    "        sections.append((prev_section, section_text))\n",
    "        prev_section = section\n",
    "    sections.append((prev_section, doc_text))\n",
    "    return sections\n",
    "    # except:\n",
    "    #     return ((\"full\", tex))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "def pass_to_ollama(prompt, model):\n",
    "    try:\n",
    "        res = ollama.generate(model=model, prompt=prompt, options={\"temperature\": 0})\n",
    "        return res[\"response\"]\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return f\"ollama error: {ex}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                    | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████▍                                                                         | 1/5 [00:58<03:54, 58.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Modem Illumination Problem', 'monotone polygons', 'number of -modems needed to illuminate', 'n/2'), ('Modem Illumination Problem', 'monotone orthogonal polygons for even n', 'number of -modems needed to illuminate', 'n/2'), ('Modem Illumination Problem', 'monotone orthogonal polygons for odd n', 'number of -modems needed to illuminate', '(n+1)/2')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████▊                                                       | 2/5 [01:44<02:33, 51.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████▌                                    | 3/5 [04:26<03:23, 101.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Segmentation', 'PubMed', 'F1', '86.19'), ('Segmentation', 'arXiv', 'F1', '85.09'), ('Summarization', 'PubMed', 'Avg-R', '38.73'), ('Summarization', 'arXiv', 'Avg-R', '36.71'), ('Segmentation', 'PubMed', 'F1', '86.47'), ('Segmentation', 'arXiv', 'F1', '85.63'), ('Summarization', 'PubMed', 'Avg-R', '38.95'), ('Summarization', 'arXiv', 'Avg-R', '36.99'), ('Segmentation', 'PubMed', 'F1', '88.74'), ('Segmentation', 'arXiv', 'F1', '87.10'), ('Summarization', 'PubMed', 'Avg-R', '39.37'), ('Summarization', 'arXiv', 'Avg-R', '37.24'), ('Segmentation', 'PubMed', 'F1', '84.93'), ('Segmentation', 'arXiv', 'F1', '77.07'), ('Summarization', 'PubMed', 'Avg-R', '38.42'), ('Summarization', 'arXiv', 'Avg-R', '36.69'), ('Segmentation', 'PubMed', 'F1', '85.26'), ('Segmentation', 'arXiv', 'F1', '78.41'), ('Summarization', 'PubMed', 'Avg-R', '38.92'), ('Summarization', 'arXiv', 'Avg-R', '36.95'), ('Segmentation', 'PubMed', 'F1', '87.19'), ('Segmentation', 'arXiv', 'F1', '81.71'), ('Summarization', 'PubMed', 'Avg-R', '39.36'), ('Summarization', 'arXiv', 'Avg-R', '37.25'), ('Informativeness', '', 'Avg.', '2.89'), ('Informativeness', '', '4/5', '10.05%'), ('Informativeness', '', '3', '21.84%'), ('Informativeness', '', 'Avg.', '2.89'), ('Diversity', '', 'Info', '82.54%'), ('Extractive Summarization', 'Scientific Articles', 'ROUGE-1', '45.2'), ('Extractive Summarization', 'Spoken Transcripts', 'ROUGE-2', '38.5'), ('Extractive Summarization', 'Scientific Articles', '', ''), ('Extractive Summarization', 'Lecture Transcripts', '', ''), ('Section Segmentation', 'Written Documents', '', ''), ('Extractive Summarization', 'Multiple Datasets', '', ''), ('Summarization', 'PubMed', 'Rouge-1', '23.76'), ('Summarization', 'PubMed', 'Rouge-2', '18.23'), ('Summarization', 'PubMed', 'Rouge-L', '250.6'), ('Summarization', 'arXiv', 'Rouge-1', '48.06'), ('Summarization', 'arXiv', 'Rouge-2', '53.26'), ('Summarization', 'arXiv', 'Rouge-L', '48.43'), ('Summarization', 'Lecture Transcripts', 'Precision', '45.1'), ('Summarization', 'Lecture Transcripts', 'Recall', '53.4'), ('Summarization', 'Lecture Transcripts', 'F-scores', '49.2'), ('Summarization', 'Lecture Transcripts', 'Rouge scores', '47.5'), ('Segmentation', '', 'F1', '87.19'), ('Summarization', '', 'ROUGE', '39.36'), ('Segmentation', '', 'F1', '81.71'), ('Summarization', '', 'ROUGE', '37.25')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████▌                  | 4/5 [05:38<01:30, 90.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mathematical Reasoning', 'GSM8k', 'Accuracy', 'SOTA'), ('Mathematical Reasoning', 'CMath', 'Accuracy', 'SOTA'), ('Mathematical Reasoning', 'KMath', 'Accuracy', 'SOTA'), ('Mathematical Reasoning', 'GSM8k', '', ''), ('Mathematical Reasoning', 'CMath', '', ''), ('Mathematical Reasoning', 'KMath', '', ''), ('Mathematics problem solving', 'KMath', 'Accuracy', 'outperforms'), ('Mathematics problem solving', 'CMath', 'Accuracy', 'close to GPT4'), ('Fine-grained results', 'CMath', 'Accuracy', 'exceeding 60%'), ('Fine-grained results', 'CMath', 'Accuracy', 'exceeding 80%'), ('Robustness evaluation', 'GSM8k robust', 'Accuracy', 'slight decrease'), ('Robustness evaluation', 'CMath distractor', 'Accuracy', 'greater than 40%'), ('Mathematical problems', '', '', ''), ('Mathematical benchmarks', 'English and Chinese', '', 'approaching GPT-4')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [06:07<00:00, 73.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Speech Recognition', 'test data', 'FRR%', '1.99'), ('Speech Recognition', 'test data', 'FRR%', '1.7'), ('Speech Recognition', 'test data', 'FRR%', '0.45'), ('Classification', '', 'FRR', '1.99'), ('Classification', '', 'FRR', '1.7'), ('Classification', '', 'FRR', '0.45'), ('Regression', '', 'MSE', ''), ('Detection', '', 'DET', ''), ('Localization', '', 'IOU vs TPR', '0.8')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>f</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline-tdms-valid_llama3_70b-05202024-223017</td>\n",
       "      <td>1503.05062</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Modem Illumination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline-tdms-valid_llama3_70b-05202024-223017</td>\n",
       "      <td>1109.0784</td>\n",
       "      <td>unanswerable\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline-tdms-valid_llama3_70b-05202024-223017</td>\n",
       "      <td>2210.16422v1</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Summarization', 'Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseline-tdms-valid_llama3_70b-05202024-223017</td>\n",
       "      <td>2310.07488v2</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Fine-grained result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baseline-tdms-valid_llama3_70b-05202024-223017</td>\n",
       "      <td>2210.15425v1</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Speech Recognition'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              run             f  \\\n",
       "0  baseline-tdms-valid_llama3_70b-05202024-223017    1503.05062   \n",
       "1  baseline-tdms-valid_llama3_70b-05202024-223017     1109.0784   \n",
       "2  baseline-tdms-valid_llama3_70b-05202024-223017  2210.16422v1   \n",
       "3  baseline-tdms-valid_llama3_70b-05202024-223017  2310.07488v2   \n",
       "4  baseline-tdms-valid_llama3_70b-05202024-223017  2210.15425v1   \n",
       "\n",
       "                                          annotation  \n",
       "0  [{'LEADERBOARD': {'Task': 'Modem Illumination ...  \n",
       "1                                     unanswerable\\n  \n",
       "2  [{'LEADERBOARD': {'Task': 'Summarization', 'Da...  \n",
       "3  [{'LEADERBOARD': {'Task': 'Fine-grained result...  \n",
       "4  [{'LEADERBOARD': {'Task': 'Speech Recognition'...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from src.dataset import TDMSDataset, PATH, UNANSWERABLE, LogResult\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def extract_tdms(tex):\n",
    "    return f'If the text reports benchmark leaderboard results, extract the reported Tasks, Datasets, Metrics and corresponding Scores.\\\n",
    "     \\n \\\n",
    "    Text: \\n {tex} \\n \\\n",
    "    Return the tasks, datasets, metrics and scores as reported in the text in a JSON array, \\\n",
    "    for example [{{\"Task\": \"example Task 1\", \"Dataset\": \"example Dataset 1\", \"Metric\": example metric 1\", \"Score\": \"score\"}}, {{\"Task\": \"example Task 1\",\"Dataset\": \"example Dataset 2\", \"Metric\": example metric 2\", \"Score\": \"score\"}}] and provide the JSON Array only. \\n \\\n",
    "    Do not include precision information in the reported score. \\\n",
    "    Entries: '\n",
    "\n",
    "def convert_tdms_to_tuple(model_output_parsed):\n",
    "    tuples = []\n",
    "    for item in model_output_parsed:\n",
    "        try:\n",
    "            t = ((item[\"Task\"], item[\"Dataset\"],item[\"Metric\"],item[\"Score\"]))\n",
    "            tuples.append(t)\n",
    "        except:\n",
    "            # parse error, ignore instance\n",
    "            pass\n",
    "    return tuples\n",
    "\n",
    "def format_tdms(tuples):\n",
    "    \"\"\"make unique, format as string\"\"\"\n",
    "    unique = set(tuples)\n",
    "    dicts = [{\"LEADERBOARD\": {\n",
    "        \"Task\": t,\n",
    "        \"Dataset\":d,\n",
    "        \"Metric\":m,\n",
    "        \"Score\":s\n",
    "    }} for t,d,m,s in unique]\n",
    "    return str(dicts)\n",
    "\n",
    "\n",
    "\n",
    "# Run on validation data\n",
    "valid_dataset = TDMSDataset(PATH.VAL)\n",
    "model = \"llama3:70b\"\n",
    "\n",
    "run_id = f\"baseline-tdms-valid_{model.replace(':', '_')}-{datetime.now().strftime('%m%d%Y-%H%M%S')}\"\n",
    "llama3_fn = lambda prompt: pass_to_ollama(prompt, model)\n",
    "\n",
    "\n",
    "logger = LogResult(run_id, do_write=True)\n",
    "indexes = len(valid_dataset)\n",
    "\n",
    "for i in tqdm(range(indexes)):\n",
    "    f, tex, ground_truth = valid_dataset.__getitem__(i)\n",
    "    found_tdms = []\n",
    "    sections = section_split(tex)\n",
    "    for section_name, section_text in sections:\n",
    "        response = llama3_fn(extract_tdms(section_text))\n",
    "\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "            parsed = convert_tdms_to_tuple(response)\n",
    "            found_tdms= [*found_tdms, *parsed]\n",
    "        except:\n",
    "            pass # no tuples found in section\n",
    "\n",
    "    # print(found_tdms)\n",
    "    if not found_tdms:\n",
    "        annotation = UNANSWERABLE # found_tdms are empty -> unanswerable\n",
    "    else:\n",
    "        # dedupe and format\n",
    "        annotation = format_tdms(found_tdms)\n",
    "    # log\n",
    "    logger.log(f, annotation)\n",
    "\n",
    "df = logger.save()\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1503.05062</td>\n",
       "      <td>unanswerable\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1109.0784</td>\n",
       "      <td>unanswerable\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2210.16422v1</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Text Summarization'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2310.07488v2</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Arithmetic Reasonin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2210.15425v1</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Keyword Spotting', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f                                       ground_truth\n",
       "0    1503.05062                                     unanswerable\\n\n",
       "1     1109.0784                                     unanswerable\\n\n",
       "2  2210.16422v1  [{'LEADERBOARD': {'Task': 'Text Summarization'...\n",
       "3  2310.07488v2  [{'LEADERBOARD': {'Task': 'Arithmetic Reasonin...\n",
       "4  2210.15425v1  [{'LEADERBOARD': {'Task': 'Keyword Spotting', ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_ground_truth = pd.DataFrame([{\"f\":f, \"ground_truth\":gt} for f, _, gt in [valid_dataset.__getitem__(i) for i in range(indexes)]])\n",
    "df_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the extracted data:\\n\\n[\\n    {\"Task\": \"Classification\", \"Dataset\": \"Internal user studies with informed consent approvals\", \"Metric\": \"Focal loss\", \"Score\": \"\"},\\n    {\"Task\": \"Regression\", \"Dataset\": \"Internal user studies with informed consent approvals\", \"Metric\": \"Mean squared error loss\", \"Score\": \"\"},\\n    {\"Task\": \"Detection\", \"Dataset\": \"Test data\", \"Metric\": \"Detection error trade-off (DET) curve\", \"Score\": \"Our method outperforms the prior work by a large margin at different operating points.\"},\\n    {\"Task\": \"\", \"Dataset\": \"\", \"Metric\": \"FRRs\", \"Score\": \"0.45\"},\\n    {\"Task\": \"\", \"Dataset\": \"\", \"Metric\": \"FRRs\", \"Score\": \"1.99\"},\\n    {\"Task\": \"\", \"Dataset\": \"\", \"Metric\": \"FRRs\", \"Score\": \"1.7\"}\\n]\\n\\nNote that the scores for the classification and regression tasks are not provided, as they are not reported in the text. The score for the detection task is a qualitative description of the performance, rather than a numerical value. The FRR scores are reported for three different methods: S1-DCNN, End-to-end trained DNN-HMM, and the authors\\' own method (prjname).'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    i, tex, jsn = valid_dataset.__getitem__(i)\n",
    "    print(jsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([(r[\"Task\"], r[\"Dataset\"],r[\"Metric\"],r[\"Score\"]) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct of success\n",
    "\n",
    "df[\"success\"] =df[\"ground_truth\"]==df[\"pred\"]\n",
    "len(df[df[\"success\"]]) / len(df)\n",
    "\n",
    "# .76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(df[\"ground_truth\"], df[\"pred\"])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random trial\n",
    "import random\n",
    "\n",
    "num_true = len(df[df[\"ground_truth\"]])\n",
    "num_tot = len(df)\n",
    "pct_true = num_true / num_tot\n",
    "\n",
    "\n",
    "df[\"random\"] = random.uniform(0, 1)\n",
    "df[\"random_success\"] = df[\"ground_truth\"] == (df[\"random\"] <= pct_true)\n",
    "len(df[df[\"random_success\"]]) / len(df)\n",
    "\n",
    "# duh..\n",
    "# .634 (random with distribution), .758 (baseline llama)\n",
    "# codalab: .53 (llama 2 baseline), .83 (top score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on train data to evaluate method\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from src.dataset import BinaryTDMSDataset, PATH, write_annotation_file, UNANSWERABLE\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "test_dataset = BinaryTDMSDataset(PATH.TEST)\n",
    "model = \"llama3:70b\"\n",
    "\n",
    "run_id = f\"baseline-test_{model.replace(':', '_')}-{datetime.now().strftime('%m%d%Y-%H%M%S')}\"\n",
    "llama3_fn = lambda prompt: pass_to_ollama(prompt, model)\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(test_dataset))):\n",
    "# for i in tqdm(range(1)):\n",
    "    f, tex, _ = test_dataset.__getitem__(i)\n",
    "    pred = section_wise_detection(tex, llama3_fn)\n",
    "    write_annotation_file(run_id, f, UNANSWERABLE if not pred else \"Something was found\")\n",
    "    # print(f\"{f}: {pred} ({ground_truth})\")\n",
    "    results.append((f, pred))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.columns = [\"file\", \"pred\"]\n",
    "df.to_feather(f\"results/{run_id}/df.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Test set evalutation\n",
    "\n",
    "# from src.dataset import BinaryTDMSDataset, PATH\n",
    "\n",
    "# test_dataset = BinaryTDMSDataset(PATH.TEST)\n",
    "\n",
    "# def get_index(folder):\n",
    "#     return [i for i, t, j in test_dataset.all_paths].index(folder)\n",
    "\n",
    "# i, tex, jsn = test_dataset.__getitem__(get_index(\"0706.0014\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = \"first_baseline_70b.feather\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_feather(res_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"pred\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
